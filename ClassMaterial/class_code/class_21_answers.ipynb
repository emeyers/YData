{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 21: Hypothesis tests continued\n",
    "\n",
    "Plan for today:\n",
    "- Review of statistical inference and hypothesis tests for a single proportion\n",
    "- Hypothesis tests for more than one proportion\n",
    "- Hypothesis tests assessing causality  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the class Jupyter setup\n",
    "\n",
    "If you have the *ydata123_2023e* environment set up correctly, you can get the class code using the code below (which presumably you've already done given that you are seeing this notebook).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(21)   # get class code    \n",
    "# YData.download.download_class_code(21, TRUE) # get the code with the answers \n",
    "\n",
    "YData.download_data(\"bta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also similar functions to download the homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YData.download.download_homework(8)  # downloads the homework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should install the YData packages by uncommenting and running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/emeyers/YData_package/tarball/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using google colabs, you should also uncomment and run the code below to mount the your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis tests\n",
    "\n",
    "In hypothesis testing, we start with a claim about a population parameter (e.g., µ = 4.2, or π = 0.25).\n",
    "\n",
    "This claim implies we should get a certain distribution of statistics, called \"The null distribution\". \n",
    "\n",
    "If our observed statistic is highly unlikely to come from the null distribution, we reject the claim. \n",
    "\n",
    "We can break down the process of running a hypothesis test into 5 steps. \n",
    "\n",
    "1. State the null and alternative hypothesis\n",
    "2. Calculate the observed statistic of interest\n",
    "3. Create the null distribution \n",
    "4. Calculate the p-value \n",
    "5. Make a decision\n",
    "\n",
    "Let's run through these steps now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis test for multiple proportions\n",
    "\n",
    "In a hypothesis test for multiple proportions, we are testing whether each proportion is equal to a particular value. I.e., we are testing whether $\\pi_1 = p_1$, $\\pi_2 = p_2$, ..., $\\pi_k = p_k$, for some proportions $p_1$, $p_2$, ..., $p_k$.\n",
    "\n",
    "A special case of this is whether all populations proportions are the same, which can be written as: $\\pi_1 = \\pi_2 = ... = \\pi_k$.\n",
    "\n",
    "\n",
    "### Motivating example: ALCU vs. Almeda County\n",
    "\n",
    "As a motivating example, let's look look at a report by the American Civil Liberties Union (ACLU) of Almada County jury selection. In particular, the ACLU claimed that jury panels in Almeda were not representative of the underlying demographics of the population of the citizens who lived there. \n",
    "\n",
    "The demographics of Almeda county, and the proportion of people selected to be on jury panels, is shown in the DataFrame below, which is based on 1453 people selected to be on jury panels. Let's use this data to run a hypothesis test to examine whether the proportion of people selected to be on jury panels is consistent with the underlying demographics of Almeda. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ethnicities = np.array(['Asian', 'Black', 'Latino', 'White', 'Other'])\n",
    "population_proportions = np.array([0.15, 0.18, 0.12, 0.54, 0.01])\n",
    "panel_proportions = np.array([0.26, 0.08, 0.08, 0.54, 0.04])\n",
    "\n",
    "\n",
    "demographics = pd.DataFrame({\"Ethnicity\": ethnicities, \n",
    "                             \"Population proportions\": population_proportions, \n",
    "                             \"Jury proportions\": panel_proportions})\n",
    "\n",
    "display(demographics)\n",
    "\n",
    "# built in pandas plotting functions\n",
    "demographics.plot.bar(\"Ethnicity\");\n",
    "plt.ylabel(\"Proportion\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: State the null and alternative hypotheses\n",
    "\n",
    "**In words** \n",
    "\n",
    "Null hypothesis: The proportions of members on the jury panels of different ethnicities match the underlying demographics. \n",
    "\n",
    "Alternative hypothesis: The proportion of at least one ethnicity does not match the underlying demographics. \n",
    "\n",
    "\n",
    "**In symbols**\n",
    "\n",
    "$H_0$: $\\pi_{Asian} = .15$,  $\\pi_{Black} = .18$,  $\\pi_{Latino} = .12$,  $\\pi_{White} = .54$,  $\\pi_{Other} = .01$\n",
    "\n",
    "$H_A$: At least one $\\pi_{i}$ is different from the values specified in the null hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the observed statistic\n",
    "\n",
    "For our observed statistic we will use the Total Variational Distance (TVD) which is defined as:  $TVD ~ = ~ \\sum_{i = 1}^{k} |\\pi_i - \\hat{p}_i |$\n",
    "\n",
    "Let's write a function `total_variation_distance(distribution_1, distribution_2)` that can calculate the TVD. We can then use this function to calculate the TVD statistic value for the jurors in Almeda county.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_distance(distribution_1, distribution_2):\n",
    "    \n",
    "    return np.sum(np.abs(distribution_1 - distribution_2))\n",
    "\n",
    "\n",
    "observed_statistic_value = total_variation_distance(panel_proportions, population_proportions)\n",
    "\n",
    "observed_statistic_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the null distribution \n",
    "\n",
    "To create the null distribution we need to simulate drawing random sample proportions from the underlying population.\n",
    "\n",
    "To do this we can generate (uniform) random numbers between 0 and 1. We can then use the `pd.cut()` function to simulate randomly selected jurors ethnicities and convert these to proportions. \n",
    "\n",
    "Once we have these proportions, we can calculate the TVD. If we repreat this process 1,000 times we can get a null distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cumulative proportions we can use to split the data into categories consistent with the null hypothesis\n",
    "\n",
    "cumulative_proportions = np.append(0, np.cumsum(population_proportions))\n",
    "\n",
    "cumulative_proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random jury panelist ethnicities\n",
    "\n",
    "num_jury_members = 1453\n",
    "\n",
    "rand_nums = np.random.rand(num_jury_members)\n",
    "\n",
    "one_sample = pd.cut(rand_nums, cumulative_proportions, labels = ethnicities, ordered = False)\n",
    "\n",
    "print(one_sample[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the proportions from our sample\n",
    "\n",
    "unique, counts = np.unique(one_sample, return_counts=True)\n",
    "\n",
    "sample_proportions = counts/sum(counts)\n",
    "\n",
    "sample_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the following steps into one function\n",
    "\n",
    "def get_sample_proportions(sample_size, true_proportions):\n",
    "    \n",
    "    cumulative_proportions = np.append(0, np.cumsum(true_proportions))\n",
    "    \n",
    "    rand_nums = np.random.rand(sample_size)\n",
    "    one_sample = pd.cut(rand_nums, cumulative_proportions)\n",
    "    unique, counts = np.unique(one_sample, return_counts=True)\n",
    "    return counts/sum(counts)\n",
    "\n",
    "    \n",
    "get_sample_proportions(1453, population_proportions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: create null distribution \n",
    "\n",
    "null_dist = []\n",
    "\n",
    "num_null_dist_points = 1000\n",
    "\n",
    "for i in range(num_null_dist_points):\n",
    "    \n",
    "    curr_sample_props = get_sample_proportions(1453, population_proportions)\n",
    "    curr_tvd = total_variation_distance(curr_sample_props, population_proportions)\n",
    "    null_dist.append(curr_tvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the null distribution as a histogram\n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the p-value\n",
    "\n",
    "The p-value is the proportion of points in the null distribution that are more extreme than the observed statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.mean(null_dist >= observed_statistic_value)\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Draw a conclusion\n",
    "\n",
    "Since the p-value is very small, it is very unlikely our statistic comes from the null distribution. Thus we can reject the null distribution and conclude that the proportion of members of different ethnicities on jury panels in Almeda do not reflect the underlying distribution of ethnicities in the population. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis test assessing causal relationships\n",
    "\n",
    "To get at causality we can run a Randomized Controlled Trial (RTC), where have of the participants are randomly assigned to a \"treatment group\" that receives an intervention and the other half of participants are put in a \"control group\" which receives a placebo. If the treatment group shows a an improvement over the control group that is larger than what is expected by chance, this indicates that the treatment **causes** an improvement. \n",
    "\n",
    "\n",
    "#### Botulinum Toxin A (BTA) as a treatment to chronic back pain\n",
    "\n",
    "A study by Foster et al (2001) examined whether Botulinum Toxin A (BTA) was an effective treatment for chronic back pain.\n",
    "\n",
    "In the study, participants were randomly assigned to be in a treatment or control group: \n",
    "- 15 in the treatment group\n",
    "- 16 in the control group (normal saline)\n",
    "\n",
    "Trials were run double-blind (neither doctors nor patients knew which group they were in)\n",
    "\n",
    "Result from the study were coded as:\n",
    "  - 1 indicates pain relief\n",
    "  - 0 indicates lack of pain relief \n",
    "\n",
    "\n",
    "Let's run a hypothesis test to see if BTA causes a decrease in back pain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: State the null and alternative hypotheses\n",
    "\n",
    "\n",
    "$H_0$: $\\pi_{treat} =  \\pi_{control}$   or    $H_0$: $\\pi_{treat} -  \\pi_{control} = 0$ \n",
    "\n",
    "$H_A$: $\\pi_{control} < \\pi_{treat}$    or    $H_0$: $\\pi_{treat} -  \\pi_{control} < 0$ \n",
    "\n",
    "\n",
    "\n",
    "Where $\\pi_{treat}$ and $\\pi_{control}$ and the population proportions of people who experienced back pain relief after receiving the BTA or control respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the observed statistic\n",
    "\n",
    "The code below loads the data from the study. We can use the difference in proportions  $\\hat{p}_{treat} - \\hat{p}_{control}$  as our observed statistic. \n",
    "\n",
    "Let's calculate the observe statistic and save it to the name `obs_stat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bta = pd.read_csv('bta.csv')\n",
    "bta.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame with the proportion of people in the treatment and control groups that have pain relief \n",
    "\n",
    "results_table = bta.groupby(\"Group\").mean()\n",
    "\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the observed statistic from our DataFrame\n",
    "\n",
    "obs_stat = results_table[\"Result\"][1] - results_table[\"Result\"][0]\n",
    "\n",
    "obs_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's write a function to make it easy to get statistic values\n",
    "\n",
    "def get_prop_diff(bta_data):\n",
    "    \n",
    "    group_means = bta_data.groupby(\"Group\").mean()\n",
    "    \n",
    "    return group_means[\"Result\"][1] - group_means[\"Result\"][0]\n",
    "\n",
    "    \n",
    "get_prop_diff(bta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the null distribution \n",
    "\n",
    "To create the null distribution, we need to create statistics consistent with the null hypothesis. \n",
    "\n",
    "In this example, if the null hypothesis was true, then there would be no difference between the treatment and control group. Thus, under the null hypothesis, we can shuffle the group labels and get equally valid statistics. \n",
    "\n",
    "Let's create one statistic consistent with the null distribution to understand the process. We can then repeat this 10,000 times to get a full null distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data \n",
    "\n",
    "shuff_bta = bta.copy()\n",
    "shuff_bta['Group'] = np.random.permutation(bta[\"Group\"])\n",
    "\n",
    "shuff_bta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one statistic consistent with the null distribution \n",
    "\n",
    "get_prop_diff(shuff_bta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create a full null distribution \n",
    "\n",
    "null_dist = []\n",
    "\n",
    "for i in range(10000):\n",
    "    \n",
    "    shuff_bta['Group'] = np.random.permutation(bta[\"Group\"])\n",
    "    \n",
    "    shuff_stat = get_prop_diff(shuff_bta)\n",
    "    \n",
    "    null_dist.append(shuff_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the null distribution \n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\");\n",
    "\n",
    "\n",
    "# put a line at the observed statistic value\n",
    "\n",
    "plt.axvline(obs_stat, color = \"red\");\n",
    "plt.xlabel(\"prop treat - prop control\");\n",
    "plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the p-value\n",
    "\n",
    "The p-value is the proportion of points in the null distribution that are more extreme than the observed statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.mean(np.array(null_dist) >= obs_stat)\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(null_dist) >= .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Draw a conclusion\n",
    "\n",
    "Since the p-value is less than the typical significance level of 0.05, we can reject the null hypothesis and conclude that BTA does **cause** pain relief at a higher rate than the placebo. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-ydata123_2023e]",
   "language": "python",
   "name": "conda-env-anaconda3-ydata123_2023e-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
