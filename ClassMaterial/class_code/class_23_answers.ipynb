{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 23: Intro to Machine Learning\n",
    "\n",
    "Plan for today:\n",
    "- Creating confidence intervals\n",
    "- Introduction to Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the class Jupyter setup\n",
    "\n",
    "If you have the *ydata123_2023e* environment set up correctly, you can get the class code using the code below (which presumably you've already done given that you are seeing this notebook).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(23)   # get class code    \n",
    "# YData.download.download_class_code(23, TRUE) # get the code with the answers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also similar functions to download the homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YData.download.download_homework(9)  # downloads the homework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should install the YData packages by uncommenting and running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/emeyers/YData_package/tarball/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using google colabs, you should also uncomment and run the code below to mount the your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using hypothesis tests to generate confidence intervals\n",
    "\n",
    "There are several methods we that can be used to calculate confidence intervals, including using a computational method called the \"bootstrap\" and using \"parametric methods\" that involve using probability distributions. If you take a traditional introductory statistics class you will learn some of these methods.\n",
    "\n",
    "Below we use a less conventional method to calculate confidence intervals by looking at all parameters values that a hypothesis test fails to reject (at the p-value < 0.05 level). As you will see, the method gives similar results to other methods, although it requires a bit more computation time.\n",
    "\n",
    "As an example, let's create a confidence interval for the population proportion of movies $\\pi$ that pass the Bechdel test. As is the case for all confidence intervals, this confidence interval gives a range of plausible values that likely contains the true population proportion $\\pi$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, let's use a function that generates a statistic p-hat that is consistent with a particular population parameter value pi\n",
    "\n",
    "def generate_prop_bechdel(n, null_prop):\n",
    "    \n",
    "    random_sample = np.random.rand(n) <= null_prop\n",
    "    return np.mean(random_sample)\n",
    "\n",
    "generate_prop_bechdel(1794, .5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function below calculates a p-value for the Bechdel data based on a particular pi value that is specified in a null hypothesis.\n",
    "# (i.e., it is a function that encapsulates the hypothesis test you ran in class 20).\n",
    "\n",
    "\n",
    "def get_Bechdel_pvalue(null_hypothesis_pi, plot_null_dist = False):\n",
    "    \n",
    "    \n",
    "    # The observed p-hat value\n",
    "    prop_passed = 803/1794\n",
    "    \n",
    "    \n",
    "    # Generate the null distribution \n",
    "    null_dist = []\n",
    "    \n",
    "    for i in range(10000):    \n",
    "        null_dist.append(generate_prop_bechdel(1794, null_hypothesis_pi))\n",
    "    \n",
    "    \n",
    "    # Calculate a \"two-tailed\" p-value which is the proportion of statistcs more extreme than the observed statistic\n",
    "    \n",
    "    statistic_deviation = np.abs(null_hypothesis_pi - prop_passed)\n",
    "    \n",
    "    pval_left = np.mean(np.array(null_dist) <= null_hypothesis_pi - statistic_deviation)\n",
    "    pval_right = np.mean(np.array(null_dist) >= null_hypothesis_pi + statistic_deviation)\n",
    "    \n",
    "    p_value = pval_left + pval_right\n",
    "\n",
    "    \n",
    "    # plot the null distribution and lines indicating values more extreme than the observed statistic \n",
    "    if plot_null_dist:\n",
    "        \n",
    "        plt.hist(null_dist, edgecolor = \"black\", bins = 30);\n",
    "        plt.axvline(null_hypothesis_pi - statistic_deviation, color = \"red\");\n",
    "        plt.axvline(null_hypothesis_pi + statistic_deviation, color = \"red\");\n",
    "        plt.axvline(null_hypothesis_pi, color = \"yellow\");\n",
    "\n",
    "        \n",
    "        plt.title(\"Pi-null is: \" + str(null_hypothesis_pi) + \"      \"  +\n",
    "                  \"p-value is: \" + str(round(p_value, 5)))\n",
    "      \n",
    "    # return the p-value\n",
    "    return p_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function with the value H0: pi = .5  (as we did in class 20)\n",
    "get_Bechdel_pvalue(.5, True)\n",
    "\n",
    "\n",
    "# test the function with the value H0: pi = .45\n",
    "plt.figure()\n",
    "get_Bechdel_pvalue(.45, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a range range of H0: pi = x  values\n",
    "\n",
    "possible_null_pis = np.round(np.arange(.4, .5, .005), 5)\n",
    "\n",
    "possible_null_pis    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get the p-value for a range of H0: pi = x  values\n",
    "\n",
    "pvalues = []\n",
    "\n",
    "for null_pi in possible_null_pis:\n",
    "    \n",
    "    curr_pvalue = get_Bechdel_pvalue(null_pi)\n",
    "    \n",
    "    pvalues.append(curr_pvalue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the p-values \n",
    "# convention calls a p-value < 0.05 is \"statistically significant\" indicating a pi imcompatible with the null hypothesis\n",
    "# our confidence interval is all pi values that are not statistically significant (i.e., pi values that are consistent with particular H0)\n",
    "\n",
    "pvalue_df = pd.DataFrame({\"pi\": possible_null_pis, \n",
    "                          \"p-values\": pvalues,\n",
    "                          \"non-significant\": np.array(pvalues) > .05})\n",
    "\n",
    "pvalue_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot p-values as as function of H0 pi's\n",
    "\n",
    "sns.relplot(pvalue_df, x = 'pi', y = 'p-values');\n",
    "plt.xticks(rotation=90);\n",
    "plt.axhline(.05, color = \"red\");\n",
    "\n",
    "plt.plot(pvalue_df['pi'], pvalue_df['non-significant'], color = \"green\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all plausible Pi values\n",
    "fail_to_reject_pis = possible_null_pis[np.array(pvalues) >= .05]\n",
    "\n",
    "fail_to_reject_pis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the CI as the max and min plausible pi values \n",
    "\n",
    "(min(fail_to_reject_pis), max(fail_to_reject_pis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the statsmodels package to compute a confidence interval for a proportion\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "ci_low, ci_upp = sm.stats.proportion_confint(803, 1794, alpha=0.05, method='normal')\n",
    "(round(ci_low, 3), round(ci_upp, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intro to Machine Learning:  Features (X) and labels (y)\n",
    "\n",
    "In supervised machine learning, we use a computer algorithm called a \"pattern classifier\" to learn relationships between a set of features X, and a label y. When the classifier is given new examples X, it can then make new predictions y. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "penguins = penguins.sample(frac = 1)\n",
    "\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore how many different members there are of each species in our data set? \n",
    "\n",
    "species_counts = penguins.groupby(\"species\").agg(count = ('island', 'count'))\n",
    "\n",
    "species_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions: \n",
    "\n",
    "1. If we had to guess the species of the penguin without knowing any of the penguin's features, species of penguin should we guess? \n",
    "A: Always guess Adelie\n",
    "\n",
    "\n",
    "2. If we were to following the optimal guessing strategy, what percent of our guess would be correct (i.e., what would our classification accuracy be)? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts['count']/sum(species_counts['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin the classification process, let's store the features (X) and the labels (y) in separate names called `X_penguin_features` and `y_penguin_labels` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features and the labels\n",
    "\n",
    "X_penguin_features = penguins[['bill_length_mm', 'bill_depth_mm','flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "y_penguin_labels = penguins['species']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. k-Nearest Neighbors classifier\n",
    "\n",
    "\n",
    "To explore classification, let's use a k-Nearest Neighbors classifier to predict the species of a penguin based on particular features the penguin has such as the penguin's bill length and body mass. \n",
    "\n",
    "Let's construct a K-Nearest Neighbor classifier (KNN) using 5 neighbors for predictions (i.e., k = 5 so we are using a 5-Nearest Neighbor classifier). \n",
    "\n",
    "We can do this using the `KNeighborsClassifier(n_neighbors = )` function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Construct a classifier a 5 nearest neighbor classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the classifier (the KNN classifier just stores the data during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “train” the classifier (which for a KNN classifier just involves memorizing the training data)\n",
    "\n",
    "knn.fit(X_penguin_features, y_penguin_labels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the classifier to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "penguin_preditions = knn.predict(X_penguin_features)\n",
    "\n",
    "penguin_preditions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the prediction (classificaton accuracy) which is the proportion of predictions that are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classification accuracy\n",
    "np.mean(penguin_preditions == y_penguin_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat our analysis with k = 1 to see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if k = 1?\n",
    "\n",
    "# construct a classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "\n",
    "# “train” the classifier (which for a KNN classifier just involves memorizing the training data)\n",
    "knn.fit(X_penguin_features, y_penguin_labels) \n",
    "\n",
    "# make predictions\n",
    "penguin_preditions = knn.predict(X_penguin_features)\n",
    "\n",
    "# get classification accuracy\n",
    "np.mean(penguin_preditions == y_penguin_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we believe we have a perfect classifier???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-validation\n",
    "\n",
    "To avoid over-fitting, we need to split our data into a training and test set. \n",
    "\n",
    "The classifier \"learns\" the relationship between features (X) and labels (y) on the **training set**.\n",
    "\n",
    "The classifier makes predictions on the features (X) of the **test set**. \n",
    "\n",
    "We compare the classifier's predictions on the test features (X) to the actual labels y, to get a more accuracy assessment of the **classification accuracy**.\n",
    "\n",
    "\n",
    "Let's try this now...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually create a training with 250 examples, and a test set that has the rest of the data\n",
    "\n",
    "X_train_manual = X_penguin_features.iloc[0:250, :]\n",
    "y_train_manual = y_penguin_labels.iloc[0:250]\n",
    "\n",
    "X_test_manual = X_penguin_features.iloc[250:, :]\n",
    "y_test_manual = y_penguin_labels.iloc[250:]\n",
    "\n",
    "\n",
    "print(X_train_manual.shape)\n",
    "print(X_test_manual.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into a training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_penguin_features,  y_penguin_labels, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# construct a classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "\n",
    "# “train” the classifier (which for a KNN classifier just involves memorizing the training data)\n",
    "knn.fit(X_train_manual, y_train_manual) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predictions\n",
    "\n",
    "penguin_preditions = knn.predict(X_test_manual)\n",
    "\n",
    "penguin_preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction accuracy \n",
    "\n",
    "np.mean(penguin_preditions == y_test_manual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier on the test set using the .score() method\n",
    "\n",
    "knn.score(X_test_manual, y_test_manual) # prediction accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we test the classifier on the training set? \n",
    "\n",
    "knn.score(X_train_manual, y_train_manual) # prediction accuracy on the training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation\n",
    "\n",
    "In k-fold cross-validation we split our data into k-parts (note, the k here has no relation to the k in k-Nearest Neighbor - it is just that k is a frequent letter to use in math to denote integer values).  \n",
    "\n",
    "To run a k-fold cross-validation analysis, we train the classifier on k-1 parts of the data and test it on the remaining part. We repeat this process k times to get k classification accuracies. We then take the average of these results as our estimate of our overall classification accuracy. \n",
    "\n",
    "We can use the scikit-learn `cross_val_score()` to easily do this...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) # construct knn classifier\n",
    "\n",
    "# do 5-fold cross-validation\n",
    "scores = cross_val_score(knn, X_penguin_features,  y_penguin_labels, cv = 5)\n",
    "\n",
    "print(scores)\n",
    "\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next class, buliding a KNN classifier ourselves..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-ydata123_2023e]",
   "language": "python",
   "name": "conda-env-anaconda3-ydata123_2023e-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
