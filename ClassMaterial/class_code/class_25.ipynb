{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 25: Unsupervised learning\n",
    "\n",
    "Plan for today:\n",
    "- Linear regression\n",
    "- Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(25)   # get class code    \n",
    "# YData.download.download_class_code(25, TRUE) # get the code with the answers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/emeyers/YData_package/tarball/master\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress ConvergenceWarning - please ignore this code \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Linear regression review\n",
    "\n",
    "In regression, we try to predict a quantitative variable y, from a set of features X. \n",
    "\n",
    "Let's explore this by predicting the body mass of penguins (in grams) from other quantitative features of a penguin (e.g., their bill and flipper sizes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins = penguins.dropna()\n",
    "penguins = penguins.sample(frac = 1)\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features and the labels\n",
    "\n",
    "X_penguin_features = penguins[['bill_length_mm', \n",
    "                               'bill_depth_mm',\n",
    "                               'flipper_length_mm']]\n",
    "\n",
    "y_penguin = penguins['body_mass_g']\n",
    "\n",
    "\n",
    "# also save the penguin species to use later\n",
    "y_penguin_species = penguins['species']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use scikit-learn to generate training and test data as we did previously for our KNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into a training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_penguin_features,  \n",
    "                                                    y_penguin, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new linear regression model, fit it to data, and make predictions. The method names are again very similar to what we used for the KNN classifier (i.e., the `fit()` and predict()` methods). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create a new linear regression moded\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "\n",
    "# fit the model to our training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make predictions of the penguins body weight on the test data\n",
    "body_mass_predictions = linear_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Use scikit-learn's mean_squared_error() function to get the RMSE\n",
    "np.sqrt(mean_squared_error(y_test, body_mass_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(linear_model, \n",
    "                         X_penguin_features,  \n",
    "                         y_penguin, \n",
    "                         cv = 5, \n",
    "                         scoring='neg_mean_squared_error')\n",
    "\n",
    "np.sqrt(np.mean(-1 * scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model equation\n",
    "\n",
    "In linear regression, our predicted $\\hat{y}$ values are given by the equation: $\\hat{y} = b_0 + b_1 x_1 + ... + + b_k x_k$.\n",
    "\n",
    "Let's fill out this equation for prediciting penguin body mass. \n",
    "\n",
    "To do this, let's start by extracting the intercept ($b_0$) and slope coefficients ($b_i's$) from our scikit-learn model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the linear regression model to our training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# get the intercept and slope coefficients\n",
    "sklearn_intercept = linear_model.intercept_\n",
    "sklearn_coefficients = linear_model.coef_\n",
    "\n",
    "# print out the coefficient values\n",
    "(sklearn_intercept, sklearn_coefficients)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these coefficient values can you write our the regression equation for predicting penguin body mass? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "$\\hat{y}_{mass} = -6680.13 + 0.5049 \\cdot x_{bill-length} + 21.6384 \\cdot x_{bill-depth} +  52.1411 \\cdot x_{lipper-length}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inference on regression coefficients\n",
    "\n",
    "We can also run inference procedures on our regression model using the statsmodel package. In particular, we can run hypothesis tests and create confidence intervals for our regression coefficents. \n",
    "\n",
    "When running a hypothesis test, our hypotheses are:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test on regression coeffients - which coefficients are statistically significantly different from zero? \n",
    "# (and confidence interval)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# add a constant value of 1 to our data\n",
    "\n",
    "\n",
    "# fit the linear regression model using the OLS function\n",
    "\n",
    "\n",
    "# get information on the regression coefficients found\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised learning: clustering\n",
    "\n",
    "We can do k-means clustering in scikit-learn using the `KMeans()` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# fit k-means with 3 clusters \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which cluster each point belongs to \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a matrix of which penguin types end up in which cluster \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# do clustering with feature normalization \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which cluster each (normalized) point belongs to\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a matrix of which penguin types end up in which cluster \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Unsupervised learning: Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "#  Ward's method adds points to a cluster that minimizes the sum of squared differences within all clusters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a dendrogram\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster points into 3 clusters \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the predicted cluster for each point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how well the clustering matches the penguin species\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chatbots \n",
    "\n",
    "Large language models (LLMs) are taking over the world. I, for one, welcome our new robot [overlords](https://www.youtube.com/watch?v=8lcUHQYhPTE).\n",
    "\n",
    "Let's explore how we can use a model from HuggingFace to create a chatbot.\n",
    "\n",
    "To do this we need to install some additional packages. I recommend cloning your Jupyter environment, and then adding these packages to the new environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from code created by Giuliano Formisano\n",
    "# Updated to work with new version of hugging face packages\n",
    "\n",
    "from transformers import pipeline, BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Option A: Using text2text-generation pipeline (single-turn):\n",
    "chatbot = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device = \"cpu\")\n",
    "\n",
    "# This uses \"mps\" by default on my mac which should be faster but there appears to be a bug in the code\n",
    "#chatbot = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "\n",
    "user_input = \"Hi! What can you do?\"\n",
    "response = chatbot(user_input, max_new_tokens=50)\n",
    "\n",
    "print(f\"User:  {user_input}\")\n",
    "print(f\"Bot:   {response[0]['generated_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for an interaction User-Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop of interaction user-chatbot\n",
    "while True:\n",
    "  user_input = input(\"You: \") # add prompt in the appearing box below\n",
    "  if user_input.lower() == \"quit\": # write \"quit\" to interrupt\n",
    "    break\n",
    "  response = chatbot(user_input) # this is a bit slow\n",
    "  print(f\"Chatbot: {response[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face also has a number of large/interesting data sets\n",
    "# (some of them controversial, and of course, one should be cautious of the veracity of all data sets)\n",
    "\n",
    "\n",
    "\n",
    "# Load a data set \n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "#emails = load_dataset(\"tensonaut/EPSTEIN_FILES_20K\", split=\"train\")\n",
    "emails = load_dataset(\"corbt/enron-emails\", split=\"train\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print type and shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert to a pandas data frame\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search for keywords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
