{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 25: Building a KNN classifier and unsupervised learning\n",
    "\n",
    "Plan for today:\n",
    "- Building a KNN classifier\n",
    "- Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the class Jupyter setup\n",
    "\n",
    "If you have the *ydata123_2023e* environment set up correctly, you can get the class code using the code below (which presumably you've already done given that you are seeing this notebook).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(25)   # get class code    \n",
    "# YData.download.download_class_code(25, TRUE) # get the code with the answers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should install the YData packages by uncommenting and running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/emeyers/YData_package/tarball/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using google colabs, you should also uncomment and run the code below to mount the your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review: Training and test sets, and KNN\n",
    "\n",
    "In supervised machine learning, we use a computer algorithm called a \"pattern classifier\" to learn relationships between a set of features X, and a label y. When the classifier is given new examples X, it can then make new predictions y. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "penguins = penguins.sample(frac = 1)\n",
    "\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features and the labels\n",
    "\n",
    "X_penguin_features = penguins[['bill_length_mm', 'bill_depth_mm','flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "y_penguin_labels = penguins['species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into a training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_penguin_features,  y_penguin_labels, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) # construct knn classifier\n",
    "\n",
    "scores = cross_val_score(knn, X_penguin_features,  y_penguin_labels, cv = 5)\n",
    "\n",
    "scores.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature normalization\n",
    "\n",
    "If you look at the features we have been using in our analyses so far, you will notice that they are on very different scales. This is quite problematic for a KNN classifier since the classifier is finding the distance between each data point, so features that have large values will dominate this distance. \n",
    "\n",
    "Let's explore the scales that different features have by looking at some descriptive statistics. In particular, let's go back to the manually created `X_train`, `X_test`, `y_train`, `y_test` to examine the scale that different features are measured on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a z-score transformation of our features which set the mean of the features to 0 and the standard deviation to 1. We can do this using the using the `StandardScaler()` object as follows: \n",
    "\n",
    "1. Create a new `StandardScaler()` object using `scalar = StandardScaler()` \n",
    "\n",
    "2. Have the `scalar` object learn the means and standard deviations of our training data by calling the `scalar.fit(X)` function on the training data.\n",
    "\n",
    "3. Use the fit `scalar` object to transform both the training and test features so that all features are on a similar scale by calling the `.transform(X)` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# learning the mean and standard deviations to scale the features\n",
    "\n",
    "scalar = StandardScaler()\n",
    "\n",
    "scalar.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score transform the features \n",
    "\n",
    "X_train_transformed = scalar.transform(X_train)\n",
    "X_test_transformed = scalar.transform(X_test)\n",
    "\n",
    "type(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at our transformed training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view descriptive statistics on the transformed features\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns = X_train.columns)\n",
    "\n",
    "X_train_transformed_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our classification accuracy changes using the z-score transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KNN classification on the normalized features\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "knn.fit(X_train_transformed, y_train)\n",
    "knn.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transform our features inside a cross-validation loop, we can set up a pipeline. This pipeline will do the following:\n",
    "\n",
    "1. It will split the data into a training and test set\n",
    "2. It will fit the transformation of the features on the training set (i.e., learn the means and standard deviations on the training set). \n",
    "3. It will apply a z-score transformation of the training and test set based on the features learned in step 2\n",
    "4. It will train the classifier on the transformed data\n",
    "5. It will measure the classification accuracy on the test data\n",
    "6. It will repeat this process k times, where k here refers to how many cross-validation splits we are using\n",
    "\n",
    "In order to do this in scikit-learn we can use a `Pipeline` object which sets up the stages of transformation and classification, along with a `KFold` object which will run the cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# create a pipeline for running cross-validation with feature normalization\n",
    "\n",
    "# components that go into the pipeline\n",
    "scalar = StandardScaler()\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "# build the pipeline\n",
    "pipeline = Pipeline([('transformer', scalar), ('estimator', knn)])\n",
    "\n",
    "# get the cross-validation scores\n",
    "scores = cross_val_score(pipeline, X_penguin_features, y_penguin_labels, cv = cv)\n",
    "\n",
    "\n",
    "# print out the mean score over the 5 cross-validation splits\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a KNN classifier\n",
    "\n",
    "So far we have used the KNN classifier (and a few other classifiers). Let's now see if we can write code that will implement the KNN classifier.\n",
    "\n",
    "We will do this by writing a several helper functions that build on each other. These functions are: \n",
    "\n",
    "1. `euclid_dist(x1, x2)`: finds the Euclidean distance between two points `x1` and `x2`\n",
    "\n",
    "2. `get_labels_and_distances(test_point, X_train_features, y_train_labels)`: This function finds the distance between a test point and all the training points. It returns a DataFrame with the distance from all training points and the training labels for each point.\n",
    "\n",
    "3. `classify_point(test_point, k, X_train_features, y_train_labels)`: Classifies which class a test point belongs to\n",
    "\n",
    "4. `classify_all_test_data(X_test_data, k, X_train_features, y_train_labels)`: Classifiers which class all test points below to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by writing a function that can get the Euclidean distance between two points `x` and `z`: \n",
    "\n",
    "$$dist(x, z) = \\sqrt{\\Sigma_{i = 1}^d (x_i - z_i)^2)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclid_dist(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "\n",
    "# test our function \n",
    "my_vec1 = np.array([1, 2, 3, 4])\n",
    "my_vec2 = np.array([2, 3, 4, 5])\n",
    "\n",
    "euclid_dist(my_vec1, my_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now write a function that returns the labels and distances \n",
    "# between a training point and all the test points\n",
    "\n",
    "\n",
    "def get_labels_and_distances(test_point, X_train_features, y_train_labels):\n",
    "    \n",
    "    the_distances = []\n",
    "    \n",
    "    # get the distance between the test point and all training points\n",
    "    for i in range(X_train_features.shape[0]):\n",
    "        the_distances.append(euclid_dist(test_point, X_train_features.iloc[i]))\n",
    "\n",
    "    \n",
    "    # add the training labels and distances on to a DataFrame \n",
    "    labels_and_distances = pd.DataFrame({'label': y_train_labels})\n",
    "    labels_and_distances['distance'] = the_distances \n",
    "\n",
    "    return labels_and_distances\n",
    "\n",
    "\n",
    "test_data_point = X_test.iloc[0]\n",
    "test_label = y_test.iloc[0]\n",
    "\n",
    "labels_and_distances = get_labels_and_distances(test_data_point, X_train, y_train)\n",
    "\n",
    "labels_and_distances.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the k closest neighbors\n",
    "\n",
    "k = 5\n",
    "\n",
    "sorted_labels_dist = labels_and_distances.sort_values(\"distance\")\n",
    "\n",
    "sorted_labels_dist = sorted_labels_dist.iloc[0:k]\n",
    "\n",
    "sorted_labels_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the majority label\n",
    "\n",
    "count_table = sorted_labels_dist.groupby(\"label\").count().reset_index()\n",
    "\n",
    "sorted_count_table = count_table.sort_values(\"distance\", ascending = False)\n",
    "\n",
    "sorted_count_table.iloc[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to do the classification on a test point \n",
    "# by putting together all the pieces\n",
    "\n",
    "def classify_point(test_point, k, X_train_features, y_train_labels):\n",
    "    \n",
    "    labels_and_distances =  get_labels_and_distances(test_point, \n",
    "                                                     X_train_features, \n",
    "                                                     y_train_labels)\n",
    "\n",
    "    sorted_labels_dist = labels_and_distances.sort_values(\"distance\")\n",
    "    sorted_labels_dist = sorted_labels_dist.iloc[0:k]\n",
    "    \n",
    "    \n",
    "    count_table = sorted_labels_dist.groupby(\"label\").count().reset_index()\n",
    "    sorted_count_table = count_table.sort_values(\"distance\", ascending = False)\n",
    "    majority_class = sorted_count_table.iloc[0][\"label\"]\n",
    "    \n",
    "    return majority_class\n",
    "\n",
    "\n",
    "\n",
    "prediction = classify_point(test_data_point, 5, X_train, y_train)\n",
    "\n",
    "print(prediction)\n",
    "\n",
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a full test set\n",
    "\n",
    "def classify_all_test_data(X_test_data, k, X_train_features, y_train_labels):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(X_test_data.shape[0]):\n",
    "        \n",
    "        curr_test_point = X_test_data.iloc[i]\n",
    "        \n",
    "        curr_prediction = classify_point(curr_test_point, \n",
    "                                         k, \n",
    "                                         X_train_features, \n",
    "                                         y_train_labels)\n",
    "        \n",
    "        predictions.append(curr_prediction)\n",
    "\n",
    "    return np.array(predictions)\n",
    "    \n",
    "    \n",
    "all_predictions = classify_all_test_data(X_test, 5, X_train, y_train)\n",
    "\n",
    "all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classification accuracy\n",
    "\n",
    "np.mean(all_predictions == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unsupervised learning: clustering\n",
    "\n",
    "We can do k-means clustering in scikit-learn using the `KMeans()` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# fit k-means with 3 clusters \n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X_penguin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which cluster each point belongs to \n",
    "\n",
    "predicted_labels = kmeans.predict(X_penguin_features)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a matrix of which penguin types end up in which cluster \n",
    "\n",
    "matrix = pd.DataFrame({'labels': predicted_labels, 'species': y_penguin_labels})\n",
    "ct = pd.crosstab(matrix['labels'], matrix['species'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# do clustering with feature normalization \n",
    "scaler = StandardScaler()\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "\n",
    "pipeline.fit(X_penguin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which cluster each (normalized) point belongs to\n",
    "\n",
    "predicted_labels2 = pipeline.predict(X_penguin_features)\n",
    "\n",
    "predicted_labels2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a matrix of which penguin types end up in which cluster \n",
    "\n",
    "matrix_new = pd.DataFrame({'labels': predicted_labels2, 'species': y_penguin_labels})\n",
    "ct_new = pd.crosstab(matrix_new['labels'], matrix_new['species'])\n",
    "print(ct_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Unsupervised learning: Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "#  Ward's method adds points to a cluster that minimizes the sum of squared differences within all clusters\n",
    "clusters = hierarchy.linkage(X_penguin_features, method=\"ward\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a dendrogram\n",
    "dendrogram = hierarchy.dendrogram(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster points into 3 clusters \n",
    "clustering_model = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
    "clustering_model.fit(X_penguin_features)\n",
    "\n",
    "# get the predicted cluster for each point\n",
    "labels = clustering_model.labels_\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how well the clustering matches the penguin species\n",
    "\n",
    "sns.relplot(X_penguin_features, \n",
    "            x='bill_length_mm', \n",
    "            y='flipper_length_mm', \n",
    "            hue=labels, \n",
    "            style = y_penguin_labels,\n",
    "            palette=\"Set2\");\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-ydata123_2023e]",
   "language": "python",
   "name": "conda-env-anaconda3-ydata123_2023e-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
