{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "---\n",
    "execute: \n",
    "  cache: false\n",
    "format: \n",
    "  pdf:\n",
    "    include-in-header: \n",
    "       text: |\n",
    "         \\newcommand{\\textasciicaret}{\\^{}}\n",
    "         \\usepackage{fvextra}\n",
    "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
    "    include-before-body:\n",
    "        text: |\n",
    "         \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{showspaces = false, showtabs = false, breaksymbolleft={}, breaklines}\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 24: Linear regression and unsupervised learning\n",
    "\n",
    "Plan for today:\n",
    "- Linear regression\n",
    "- Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(24)   # get class code    \n",
    "# YData.download.download_class_code(24, TRUE) # get the code with the answers \n",
    "\n",
    "# YData.download.download_homework(9)  # downloads the homework \n",
    "\n",
    "# project review template\n",
    "# YData.download.download_class_file('reviewer_template.ipynb', 'homework')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/emeyers/YData_package/tarball/master\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress ConvergenceWarning - please ignore this code \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Feature normalization\n",
    "\n",
    "If you look at the features we have been using in our analyses, you will notice that they are on very different scales. This is quite problematic for a KNN classifier since the classifier is finding the distance between each data point, so features that have large values will dominate this distance. \n",
    "\n",
    "Let's explore the scales that different features have by looking at some descriptive statistics. In particular, let's go back to the manually created `X_train`, `X_test`, `y_train`, `y_test` to examine the scale that different features are measured on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create the training and test splots of the data using train_test_split\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m(X_penguin_features,  \n\u001b[32m      3\u001b[39m                                                     y_penguin_labels, \n\u001b[32m      4\u001b[39m                                                     random_state = \u001b[32m0\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Get summary statistics of the training data using the .describe() method\u001b[39;00m\n\u001b[32m      7\u001b[39m X_train.describe()\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the training and test splots of the data using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_penguin_features,  \n",
    "                                                    y_penguin_labels, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Get summary statistics of the training data using the .describe() method\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a z-score transformation of our features which set the mean of the features to 0 and the standard deviation to 1. We can do this using the using the `StandardScaler()` object as follows: \n",
    "\n",
    "1. Create a new `StandardScaler()` object using `scaler = StandardScaler()` \n",
    "\n",
    "2. Have the `scaler` object learn the means and standard deviations of our training data by calling the `scaler.fit(X)` function on the training data.\n",
    "\n",
    "3. Use the fit `scaler` object to transform both the training and test features so that all features are on a similar scale by calling the `.transform(X)` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# learning the mean and standard deviations to scale the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score transform the features \n",
    "\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "type(X_test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at our transformed training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view descriptive statistics on the transformed features\n",
    "\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, \n",
    "                                      columns = X_train.columns)\n",
    "\n",
    "X_train_transformed_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our classification accuracy changes using the z-score transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply KNN classification on the normalized features\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "knn.fit(X_train_transformed, y_train)\n",
    "knn.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to transform our features inside a cross-validation loop, we can set up a pipeline. This pipeline will do the following:\n",
    "\n",
    "1. It will split the data into a training and test set\n",
    "2. It will fit the transformation of the features on the training set (i.e., learn the means and standard deviations on the training set). \n",
    "3. It will apply a z-score transformation of the training and test set based on the features learned in step 2\n",
    "4. It will train the classifier on the transformed data\n",
    "5. It will measure the classification accuracy on the test data\n",
    "6. It will repeat this process k times, where k here refers to how many cross-validation splits we are using\n",
    "\n",
    "In order to do this in scikit-learn we can use a `Pipeline` object which sets up the stages of transformation and classification. We can then use the `cross_val_score()` function to run cross-validation on this pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# create a pipeline for running cross-validation with feature normalization\n",
    "\n",
    "# components that go into the pipeline\n",
    "scaler = StandardScaler()\n",
    "knn = KNeighborsClassifier(n_neighbors = 1) \n",
    "\n",
    "# build the pipeline\n",
    "#pipeline = Pipeline([('transformer', scaler), ('estimator', knn)])\n",
    "pipeline = make_pipeline(scaler, knn)\n",
    "\n",
    "# get the cross-validation scores\n",
    "scores = cross_val_score(pipeline, \n",
    "                         X_penguin_features, \n",
    "                         y_penguin_labels, \n",
    "                         cv = 5)\n",
    "\n",
    "\n",
    "# print out the mean score over the 5 cross-validation splits\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression\n",
    "\n",
    "In regression, we try to predict a quantitative variable y, from a set of features X. \n",
    "\n",
    "Let's explore this by predicting the body mass of penguins (in grams) from other quantitative features of a penguin (e.g., their bill and flipper sizes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset(\"penguins\")\n",
    "\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "penguins = penguins.sample(frac = 1)\n",
    "\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the features and the labels\n",
    "\n",
    "X_penguin_features = penguins[['bill_length_mm', \n",
    "                               'bill_depth_mm',\n",
    "                               'flipper_length_mm']]\n",
    "\n",
    "y_penguin = penguins['body_mass_g']\n",
    "\n",
    "\n",
    "# also save the penguin species to use later\n",
    "y_penguin_species = penguins['species']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use scikit-learn to generate training and test data as we did previously for our KNN classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into a training and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_penguin_features,  \n",
    "                                                    y_penguin, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new linear regression model, fit it to data, and make predictions. The method names are again very similar to what we used for the KNN classifier (i.e., the `fit()` and predict()` methods). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a new linear regression modedl\n",
    "linear_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to our training data\n",
    "\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions of the penguins body weight on the test data\n",
    "\n",
    "body_mass_predictions = linear_model.predict(X_test)\n",
    "body_mass_predictions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assess the accuracy of our predictons using the root mean squared error which is defined as: \n",
    "\n",
    "$$RMSE = \\sqrt{ \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "Here $\\hat{y}$ is the predictions made by our linear model on the test data (i.e., the predicted body weight) and y is the actual body weights for the points in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the RMSE on the test data\n",
    "\n",
    "RMSE = np.sqrt(np.mean((y_test - body_mass_predictions)**2))\n",
    "\n",
    "RMSE   # shows (in grams) how for our predictions of penguin mass is typically off by\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use scikit-learn's `mean_squared_error()` to get the MSE, and we can use the `cross_val_score` to run k-fold cross-validation (again, in a very similar way to what we did for our KNN classifier). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use scikit-learn's mean_squared_error() function to get the RMSE\n",
    "np.sqrt(mean_squared_error(y_test, body_mass_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(linear_model, \n",
    "                         X_penguin_features,  \n",
    "                         y_penguin, \n",
    "                         cv = 5, \n",
    "                         scoring='neg_mean_squared_error')\n",
    "\n",
    "np.sqrt(np.mean(-1 * scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model equation\n",
    "\n",
    "In linear regression, our predicted $\\hat{y}$ values are given by the equation: $\\hat{y} = b_0 + b_1 x_1 + ... + + b_k x_k$.\n",
    "\n",
    "Let's fill out this equation for prediciting penguin body mass. \n",
    "\n",
    "To do this, let's start by extracting the intercept ($b_0$) and slope coefficients ($b_i's$) from our scikit-learn model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the linear regression model to our training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# get the intercept and slope coefficients\n",
    "sklearn_intercept = linear_model.intercept_\n",
    "sklearn_coefficients = linear_model.coef_\n",
    "\n",
    "# print out the coefficient values\n",
    "(sklearn_intercept, sklearn_coefficients)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these coefficient values can you write our the regression equation for predicting penguin body mass? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "$\\hat{y}_{mass} = -6680.13 + 0.5049 \\cdot x_{bill-length} + 21.6384 \\cdot x_{bill-depth} +  52.1411 \\cdot x_{lipper-length}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing our own prediction function\n",
    "\n",
    "Let's also write our own function called `get_predictions(b0_intercept, b_coefficients, X_data)` that takes the coefficient values and X values and returns predicted $\\hat{y}$ values for each X value. In particular, the arguments to the function are:\n",
    "\n",
    "1. `b0_intercept`: The linear regression intercept\n",
    "2. `b_coefficients`: The linear regression slope coefficients\n",
    "3. `X_data`: The X data values \n",
    "\n",
    "The returned value is a numpy ndarray of predictions for each X data point. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to get the predictions\n",
    "def get_predictions(b0_intercept, b_coefficients, X_data):\n",
    "    return np.sum(X_data.to_numpy() * b_coefficients, axis = 1) + b0_intercept \n",
    "\n",
    "\n",
    "# get the predicted values on the test data\n",
    "predicted_vals = get_predictions(sklearn_intercept, sklearn_coefficients, X_test)\n",
    "\n",
    "\n",
    "# see the it matches the scikit-learn predictions\n",
    "predicted_vals_sklearn = linear_model.predict(X_test)\n",
    "predicted_vals == predicted_vals_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Inference on regression coefficients\n",
    "\n",
    "We can also run inference procedures on our regression model using the statsmodel package. In particular, we can run hypothesis tests and create confidence intervals for our regression coefficents. \n",
    "\n",
    "When running a hypothesis test, our hypotheses are:\n",
    "\n",
    "$H_0: \\beta_i = 0$  \n",
    "$H_A: \\beta_i \\ne 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis test on regression coeffients - which coefficients are statistically significantly different from zero? \n",
    "# (and confidence interval)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# add a constant value of 1 to our data\n",
    "X_train_with_constant = sm.add_constant(X_train) \n",
    "\n",
    "# fit the linear regression model using the OLS function\n",
    "sm_linear_model = sm.OLS(y_train, X_train_with_constant).fit()\n",
    "\n",
    "# get information on the regression coefficients found\n",
    "print(sm_linear_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unsupervised learning: clustering\n",
    "\n",
    "We can do k-means clustering in scikit-learn using the `KMeans()` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# fit k-means with 3 clusters \n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X_penguin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which cluster each point belongs to \n",
    "\n",
    "predicted_labels = kmeans.predict(X_penguin_features)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a matrix of which penguin types end up in which cluster \n",
    "\n",
    "matrix = pd.DataFrame({'labels': predicted_labels, \n",
    "                       'species': y_penguin_species})\n",
    "\n",
    "ct = pd.crosstab(matrix['labels'], matrix['species'])\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# do clustering with feature normalization \n",
    "scaler = StandardScaler()\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "\n",
    "pipeline.fit(X_penguin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see which cluster each (normalized) point belongs to\n",
    "\n",
    "predicted_labels2 = pipeline.predict(X_penguin_features)\n",
    "\n",
    "predicted_labels2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a matrix of which penguin types end up in which cluster \n",
    "\n",
    "matrix_new = pd.DataFrame({'labels': predicted_labels2, \n",
    "                           'species': y_penguin_species})\n",
    "\n",
    "ct_new = pd.crosstab(matrix_new['labels'], \n",
    "                     matrix_new['species'])\n",
    "print(ct_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Unsupervised learning: Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "#  Ward's method adds points to a cluster that minimizes the sum of squared differences within all clusters\n",
    "clusters = hierarchy.linkage(X_penguin_features, method=\"ward\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a dendrogram\n",
    "dendrogram = hierarchy.dendrogram(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster points into 3 clusters \n",
    "clustering_model = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
    "clustering_model.fit(X_penguin_features)\n",
    "\n",
    "# get the predicted cluster for each point\n",
    "labels = clustering_model.labels_\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how well the clustering matches the penguin species\n",
    "\n",
    "sns.relplot(X_penguin_features, \n",
    "            x='bill_length_mm', \n",
    "            y='flipper_length_mm', \n",
    "            hue=labels, \n",
    "            style = y_penguin_species,\n",
    "            palette=\"Set2\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# You can run this code to covert this Jupyter notebook into a pdf\n",
    "!quarto render class_24_answers.ipynb --cache-refresh --to pdf "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
