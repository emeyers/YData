{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 20: Introduction to statistical inference\n",
    "\n",
    "Plan for today:\n",
    "- Project topics: dealing with dates\n",
    "- Review statistical inference\n",
    "- Continue with hypothesis tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on the class Jupyter setup\n",
    "\n",
    "If you have the *ydata123_2023e* environment set up correctly, you can get the class code using the code below (which presumably you've already done given that you are seeing this notebook).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import YData\n",
    "\n",
    "# YData.download.download_class_code(20)   # get class code    \n",
    "# YData.download.download_class_code(20, TRUE) # get the code with the answers \n",
    "\n",
    "YData.download_data(\"dow.csv\")\n",
    "YData.download_data(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also similar functions to download the homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YData.download.download_class_file('project_template.ipynb', 'homework')  # downloads the class project template (hopefully you've already done this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using colabs, you should install polars and the YData packages by uncommenting and running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install https://github.com/emeyers/YData_package/tarball/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using google colabs, you should also uncomment and run the code below to mount the your google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dealing with dates\n",
    "\n",
    "Often there are columns in a DataFrame that contain information about dates. In order to be able to process this information effectively, it is useful to convert them to \"datetime\" types. \n",
    "\n",
    "Let's start by loading out DOW data. If you look at the dtypes of the columns, you will notice that the Date column is an object type. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dow1 = pd.read_csv(\"dow.csv\")\n",
    "display(dow1.head(3))\n",
    "print(dow1.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in the the Date column as a datetime type, we can set the `parse_dates` argument to a list of the columns that have dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow2 = pd.read_csv(\"dow.csv\", parse_dates= [0])\n",
    "display(dow2.head(3))\n",
    "print(dow2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To only get data in a certain data range, we can create a datetime name and then use it to filter out data to get data in a particular range of dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Create a datatime object (arguments are year, month and day)\n",
    "\n",
    "my_date = datetime.datetime(2000, 1, 1)\n",
    "\n",
    "my_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series of Booleans indicating which dates are earlier than my_date\n",
    "\n",
    "dow2[\"Date\"] < my_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data before our date\n",
    "\n",
    "dow3 = dow2.copy()\n",
    "\n",
    "dow3 = dow3[dow3[\"Date\"] < my_date]\n",
    "\n",
    "dow3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review of statistical inference\n",
    "\n",
    "In statistical inference we use a smaller sample of data to make claims about a larger population of data. \n",
    "\n",
    "As an example, let's look at the [2020 election](https://www.cookpolitical.com/2020-national-popular-vote-tracker) between Donald Trump and Joe Biden, and let's focus on the results from the state of Georgia. After all the votes had been counted, the resuts showed that:\n",
    "\n",
    "- Biden received 2,461,854 votes\n",
    "- Trump received 2,473,633 votes\n",
    "\n",
    "Since we have all the votes on election data, we can precisely calculate the population parameter of the proportion of votes that Biden received, which we will denote with the symbol $\\pi_{Biden}$. \n",
    "\n",
    "Let's create names `num_trump_votes` and `num_biden_votes`, and calculate `true_prop_Biden` which is the value $\\pi_{Biden}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trump_votes = 2461854  # 2,461,854\n",
    "num_biden_votes = 2473633  # 2,473,633\n",
    "\n",
    "true_prop_Biden = num_biden_votes/(num_biden_votes + num_trump_votes)\n",
    "\n",
    "true_prop_Biden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a DataFrame called `georgia_df` that captures these election results. Each row in the DataFrame represents a votes. The column `Voted Biden` is `True` if a voter voted for Biden and `False` if the voter voted for Trump. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling distribution\n",
    "\n",
    "Suppose 10,000 polls were conducted. How many of them would show that Biden would get the majority of the vote? \n",
    "\n",
    "Let's simulate this \"sampling distribution\" of statistics using the \"faster way\" we discussed last class.  \n",
    "\n",
    "To do this we can generate random numbers between 0 and 1. If a number is less than the value of $\\pi_{Biden}$, then it is a vote for Biden (i.e., a `True` value) otherwise it is a vote for Trump (`False` value). \n",
    "\n",
    "Let's generate one sample of 1,000 voters...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# usse `np.random.rand()` to generate 1,000 numbers between 0 and 1\n",
    "thousand_random_nums = np.random.rand(1000)\n",
    "\n",
    "print(thousand_random_nums[0:5])\n",
    "\n",
    "# visualize these 1,000 numbers as a histogram\n",
    "plt.hist(thousand_random_nums, edgecolor = \"black\");\n",
    "\n",
    "# add a red vertical line at the value of pi_biden\n",
    "plt.axvline(true_prop_Biden, color = \"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a vector of Booleans (True = vote for Biden, False = vote for Trump)\n",
    "\n",
    "voter_sample = thousand_random_nums <= true_prop_Biden\n",
    "\n",
    "voter_sample[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of votes for Biden in our sample\n",
    "\n",
    "# method 1\n",
    "sample_biden_prop = np.sum(voter_sample)/1000\n",
    "print(sample_biden_prop)\n",
    "\n",
    "# method 2\n",
    "sample_biden_prop2 = np.mean(voter_sample)\n",
    "print(sample_biden_prop2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate proportion of Biden voters based on a poll\n",
    "\n",
    "def generate_prop_biden(poll_size):\n",
    "    random_sample = np.random.rand(poll_size) <= true_prop_Biden\n",
    "    return np.mean(random_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# sampling distribution of many polls conducted\n",
    "\n",
    "sample_size = 1000\n",
    "num_simulations = 10000\n",
    "\n",
    "sampling_dist = []\n",
    "\n",
    "for i in range(num_simulations):\n",
    "    prop_biden = generate_prop_biden(sample_size)\n",
    "    sampling_dist.append(prop_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sampling_dist, edgecolor = \"black\", bins = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hypothesis test for a single proportion\n",
    "\n",
    "In hypothesis testing, we start with a claim about a population parameter (e.g., µ = 4.2, or π = 0.25).\n",
    "\n",
    "This claim implies we should get a certain distribution of statistics, called \"The null distribution\". \n",
    "\n",
    "If our observed statistic is highly unlikely to come from the null distribution, we reject the claim. \n",
    "\n",
    "We can break down the process of running a hypothesis test into 5 steps. \n",
    "\n",
    "1. State the null and alternative hypothesis\n",
    "2. Calculate the observed statistic of interest\n",
    "3. Create the null distribution \n",
    "4. Calculate the p-value \n",
    "5. Make a decision\n",
    "\n",
    "Let's run through these steps now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: State the null and alternative hypothesis\n",
    "\n",
    "$H_0: \\pi = 0.5$\n",
    "\n",
    "$H_A: \\pi < 0.5$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate the observed statistic of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce data to a smaller number of columns: \"title\" and \"binary\"\n",
    "\n",
    "movies_smaller = movies[[\"title\", \"binary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the proportion of movies that pass the Bechdel test\n",
    "\n",
    "booleans_passed = movies_smaller[\"binary\"] == \"PASS\"\n",
    "\n",
    "prop_passed = np.mean(booleans_passed)\n",
    "\n",
    "prop_passed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Create the null distribution \n",
    "\n",
    "We need to create a null distribution, which is the distribution of statistics we would expect to get if the null hypothesis is true. \n",
    "\n",
    "**Question**: about what percent of the movies would we expect to pass the Bechdel test if the null distribution was true? \n",
    "\n",
    "**Answer**: 50%\n",
    "\n",
    "Let's create simulated data that is consistent with this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate one proportion consistent with the null hypothesis\n",
    "\n",
    "n = movies.shape[0]\n",
    "print(n)\n",
    "\n",
    "null_sample = np.random.rand(n) < .5\n",
    "\n",
    "np.mean(null_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a function to generate a proportions consistent with a null hypothesis\n",
    "\n",
    "def generate_prop_bechdel(n, null_prop):\n",
    "    random_sample = np.random.rand(n) <= null_prop\n",
    "    return np.mean(random_sample)\n",
    "\n",
    "generate_prop_bechdel(1794, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a null distribution \n",
    "\n",
    "null_dist = []\n",
    "\n",
    "for i in range(10000):    \n",
    "    null_dist.append(generate_prop_bechdel(1794, .5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the null distribution \n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\", bins = 20) #, range = (.4, .6));\n",
    "plt.plot(prop_passed, 30, '.', markersize = 30, color = \"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Calculate the p-value \n",
    "\n",
    "Calculate the proportion of points in the null distribution that are more extreme than the observed statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-value\n",
    "\n",
    "stats_more_extreme = np.array(null_dist) <= prop_passed\n",
    "\n",
    "print(stats_more_extreme[0:5])\n",
    "\n",
    "p_value = np.mean(stats_more_extreme)\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Make a decision\n",
    "\n",
    "Since the p-value is very small (essentially zero) it is very unlikely that our statistic come from the null distribution. Thus we will reject the null hypothesis and conclude that less than 50% of movies pass the Bechdel test. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hypothesis test for multiple proportions\n",
    "\n",
    "In a hypothesis test for multiple proportions, we are testing whether each proportion is equal to a particular value. I.e., we are testing whether $\\pi_1 = p_1$, $\\pi_2 = p_2$, ..., $\\pi_k = p_k$, for some proportions $p_1$, $p_2$, ..., $p_k$.\n",
    "\n",
    "A special case of this is whether all populations proportions are the same, which can be written as: $\\pi_1 = \\pi_2 = ... = \\pi_k$.\n",
    "\n",
    "\n",
    "### Movivating example: ALCU vs. Almeda County\n",
    "\n",
    "As a motivating example, let's look look at a report by the American Civil Liberties Union (ACLU) of Almada County jury selection. In particular, the ACLU claimed that jury panels in Almeda were not representative of the underlying demographics of the population of the citizens who lived there. \n",
    "\n",
    "The demographics of Almeda county, and the proportion of people selected to be on jury panels, is shown in the DataFrame below, which is based on 1453 people selected to be on jury panels. Let's use this data to run a hypothesis test to examine whether the proportion of people selected to be on jury panels is consistent with the underlying demographics of Almeda. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ethnicities = np.array(['Asian', 'Black', 'Latino', 'White', 'Other'])\n",
    "population_proportions = np.array([0.15, 0.18, 0.12, 0.54, 0.01])\n",
    "panel_proportions = np.array([0.26, 0.08, 0.08, 0.54, 0.04])\n",
    "\n",
    "\n",
    "demographics = pd.DataFrame({\"Ethnicity\": ethnicities, \n",
    "                             \"Population proportions\": population_proportions, \n",
    "                             \"Jury proportions\": panel_proportions})\n",
    "\n",
    "display(demographics)\n",
    "\n",
    "# built in pandas plotting functions\n",
    "demographics.plot.bar(\"Ethnicity\");\n",
    "plt.ylabel(\"Proportion\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: State the null and alternative hypotheses\n",
    "\n",
    "**In words** \n",
    "\n",
    "Null hypothesis: The proportions of members on the jury panels of different ethnicities match the underlying demographics. \n",
    "\n",
    "Alternative hypothesis: The proportion of at least one ethnicity does not match the underlying demographics. \n",
    "\n",
    "\n",
    "**In symbols**\n",
    "\n",
    "$H_0$: $\\pi_{Asian} = .15$,  $\\pi_{Black} = .18$,  $\\pi_{Latino} = .12$,  $\\pi_{White} = .54$,  $\\pi_{Other} = .01$\n",
    "\n",
    "$H_A$: At least one $\\pi_{i}$ is different from the values specified in the null hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculate the observed statistic\n",
    "\n",
    "For our observed statistic we will use the Total Variational Distance (TVD) which is defined as:  $TVD ~ = ~ \\sum_{i = 1}^{k} |\\pi_i - \\hat{p}_i |$\n",
    "\n",
    "Let's write a function `total_variation_distance(distribution_1, distribution_2)` that can calculate the TVD. We can then use this function to calculate the TVD statistic value for the jurors in Almeda county.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_distance(distribution_1, distribution_2):\n",
    "    \n",
    "    return np.sum(np.abs(distribution_1 - distribution_2))\n",
    "\n",
    "\n",
    "observed_statistic_value = total_variation_distance(panel_proportions, population_proportions)\n",
    "\n",
    "observed_statistic_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the null distribution \n",
    "\n",
    "To create the null distribution we need to simulate drawing random sample proportions from the underlying population.\n",
    "\n",
    "To do this we can generate (uniform) random numbers between 0 and 1. We can then use the `pd.cut()` function to simulate randomly selected jurors ethnicities and convert these to proportions. \n",
    "\n",
    "Once we have these proportions, we can calculate the TVD. If we repreat this process 1,000 times we can get a null distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cumulative proportions we can use to split the data into categories consistent with the null hypothesis\n",
    "\n",
    "cumulative_proportions = np.append(0, np.cumsum(population_proportions))\n",
    "\n",
    "cumulative_proportions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random jury panelist ethnicities\n",
    "\n",
    "num_jury_members = 1453\n",
    "\n",
    "rand_nums = np.random.rand(num_jury_members)\n",
    "\n",
    "one_sample = pd.cut(rand_nums, cumulative_proportions, labels = ethnicities, ordered = False)\n",
    "\n",
    "print(one_sample[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the proportions from our sample\n",
    "\n",
    "unique, counts = np.unique(one_sample, return_counts=True)\n",
    "\n",
    "sample_proportions = counts/sum(counts)\n",
    "\n",
    "sample_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert the following steps into one function\n",
    "\n",
    "def get_sample_proportions(sample_size, true_proportions):\n",
    "    \n",
    "    cumulative_proportions = np.append(0, np.cumsum(true_proportions))\n",
    "    \n",
    "    rand_nums = np.random.rand(sample_size)\n",
    "    one_sample = pd.cut(rand_nums, cumulative_proportions)\n",
    "    unique, counts = np.unique(one_sample, return_counts=True)\n",
    "    return counts/sum(counts)\n",
    "\n",
    "    \n",
    "get_sample_proportions(1453, population_proportions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: create null distribution \n",
    "\n",
    "null_dist = []\n",
    "\n",
    "num_null_dist_points = 1000\n",
    "\n",
    "for i in range(num_null_dist_points):\n",
    "    \n",
    "    curr_sample_props = get_sample_proportions(1453, population_proportions)\n",
    "    curr_tvd = total_variation_distance(curr_sample_props, population_proportions)\n",
    "    null_dist.append(curr_tvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the null distribution as a histogram\n",
    "\n",
    "plt.hist(null_dist, edgecolor = \"black\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Calculate the p-value\n",
    "\n",
    "The p-value is the proportion of points in the null distribution that are more extreme than the observed statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.mean(null_dist >= observed_statistic_value)\n",
    "\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Draw a conclusion\n",
    "\n",
    "Since the p-value is very small, it is very unlikely our statistic comes from the null distribution. Thus we can reject the null distribution and conclude that the proportion of members of different ethnicities on jury panels in Almeda do not reflect the underlying distribution of ethnicities in the population. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-ydata123_2023e]",
   "language": "python",
   "name": "conda-env-anaconda3-ydata123_2023e-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
